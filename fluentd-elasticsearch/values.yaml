image:
  repository: gcr.io/google-containers/fluentd-elasticsearch
## Specify an imagePullPolicy (Required)
## It's recommended to change this to 'Always' if the image tag is 'latest'
## ref: http://kubernetes.io/docs/user-guide/images/#updating-images
  tag: v2.3.1
  pullPolicy: IfNotPresent

## Configure resource requests and limits
## ref: http://kubernetes.io/docs/user-guide/compute-resources/
##
resources: {}
  # limits:
  #   cpu: 100m
  #   memory: 500Mi
  # requests:
  #   cpu: 100m
  #   memory: 200Mi

elasticsearch:
  host: 'elasticsearch-elasticsearch-cluster'
  port: 9200
  buffer_chunk_limit: 2M
  buffer_queue_limit: 8

rbac:
  create: true

serviceAccount:
  # Specifies whether a ServiceAccount should be created
  create: true
  # The name of the ServiceAccount to use.
  # If not set and create is true, a name is generated using the fullname template
  name:

livenessProbe:
  enabled: true

annotations: {}
  # prometheus.io/scrape: "true"
  # prometheus.io/port: "24231"

tolerations: {}
  # - key: node-role.kubernetes.io/master
  #   operator: Exists
  #   effect: NoSchedule

nodeSelector: {}

service: {}
  # type: ClusterIP
  # ports:
  #   - name: "monitor-agent"
  #     port: 24231

configMaps:
  containers.input.conf: |-
    <source>
      @id fluentd-containers.log
      @type tail
      path /var/log/containers/*.log
      pos_file /var/log/fluentd-containers.log.pos
      time_format %Y-%m-%dT%H:%M:%S.%NZ
      tag raw.kubernetes.*
      format json
      read_from_head true
    </source>

  output.conf: |
    # Enriches records with Kubernetes metadata
    <filter raw.kubernetes.**>
      @type kubernetes_metadata
    </filter>

    <filter raw.kubernetes.var.log.containers.**logs**>
      @type parser
      format json
      replace_invalid_sequence true
      emit_invalid_record_to_error true
      suppress_parse_error_log false
      key_name log
      reserve_data true
      reserve_time true
      # hash_value_field log
      time_key occurred
    </filter>

    # <filter raw.kubernetes.var.log.containers.**logs**>
    #   @type parser
    #   format json
    #   replace_invalid_sequence true
    #   emit_invalid_record_to_error true
    #   suppress_parse_error_log false
    #   key_name "log"
    #   reserve_data true
    #   reserve_time true
    #   time_key occurred
    #   <parse>
    #     time_key occurred
    #     @type json
    #   </parse>
    # </filter>

    # <match **>
    #     @type elasticsearch
    #     @id out_es
    #     @log_level info
    #     include_tag_key true
    #     host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
    #     port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
    #     scheme "#{ENV['FLUENT_ELASTICSEARCH_SCHEME'] || 'http'}"
    #     ssl_verify "#{ENV['FLUENT_ELASTICSEARCH_SSL_VERIFY'] || 'true'}"
    #     user "#{ENV['FLUENT_ELASTICSEARCH_USER']}"
    #     password "#{ENV['FLUENT_ELASTICSEARCH_PASSWORD']}"
    #     reload_connections "#{ENV['FLUENT_ELASTICSEARCH_RELOAD_CONNECTIONS'] || 'true'}"
    #     logstash_prefix "#{ENV['FLUENT_ELASTICSEARCH_LOGSTASH_PREFIX'] || 'fluentd'}"
    #     # template_name 'template_1'
    #     # template_file /parsed_config/template.json
    #     logstash_format true
    #     logstash_prefix fluentd
    #     # logstash_dateformat %Y.%m.%d
    #     max_retry_wait 30
    #     disable_retry_limit
    #     num_threads 8
    #     # pipeline json_log
    #     # time_key @timestamp
    #     # time_key_format %Y-%m-%dT%H:%M:%S.%N%z
    #     # include_timestamp true
    # </match>

    <match **>
      @id elasticsearch
      @type elasticsearch
      @log_level info
      include_tag_key true
      type_name fluentd
      host "#{ENV['OUTPUT_HOST']}"
      port "#{ENV['OUTPUT_PORT']}"
      logstash_format true
      logstash_prefix fluentd
      <buffer>
        @type file
        path /var/log/fluentd-buffers/kubernetes.system.buffer
        flush_mode interval
        retry_type exponential_backoff
        flush_thread_count 2
        flush_interval 5s
        retry_forever
        retry_max_interval 30
        chunk_limit_size "#{ENV['OUTPUT_BUFFER_CHUNK_LIMIT']}"
        queue_limit_length "#{ENV['OUTPUT_BUFFER_QUEUE_LIMIT']}"
        overflow_action drop_oldest_chunk
      </buffer>
    </match>

# extraVolumes:
#   - name: es-certs
#     secret:
#       defaultMode: 420
#       secretName: es-certs
# extraVolumeMounts:
#   - name: es-certs
#     mountPath: /certs
#     readOnly: true
